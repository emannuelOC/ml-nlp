<!--# Proposta de conte√∫do para o artigo

* Redes Neurais Artificiais
* Deep Learning
* RNN
* LSTM
* RNN e LSTM em Deep Learning
* Representa√ß√£o vetorial de palavras
* Word2Vec
* GloVe
* Comparativo
* Aplica√ß√£o (an√°lise de sentimento)-->

# Reuni√£o 19/04

## Poster

* Poster menor
* Pensar no modelo

## Manual instala√ß√µes

* Colocar a √∫ltima vers√£o pras depend√™ncias
* Explicitar arquitetura (mac, linux, etc)
* Data
* Pip para come√ßar (antes das outras)
* Testar se est√° instalado
* Ver manual do Bonlee
* exemplo de uso (desde caminhar at√© o diret√≥rio, ativar um ambiente, etc.)


# Proposta para o poster

* Representa√ß√£o de palavras
* Limita√ß√µes das representa√ß√µes discretas
* Representa√ß√µes distribu√≠das
* word2vec
* GloVe
* Compara√ß√£o qualitativa
* Compara√ß√£o quantitativa (instr√≠nseca - dados do google - e extr√≠nseca - alguma tarefa ???)

## Representa√ß√£o de palavras

Representar uma palavra de modo que esta possa servir de input a um sistema computacional n√£o √© uma tarefa √≥bvia. O pr√≥prio conceito de _palavra_ √© complexo em si mesmo. Qualquer que seja a representa√ß√£o escolhida, √© bem prov√°vel que certas nuances ser√£o deixadas de lado e alguns "detalhes" ser√£o perdidos.

## Representa√ß√µes discretas

Historicamente as palavras t√™m sido representadas de modo discreto. Um exemplo t√≠pico dessas representa√ß√µes √© o _one-hot vector_. Nessa representa√ß√£o, h√° um vetor n-dimensional (onde n √© o tamanho do vocabul√°rio) com 0 em todas as posi√ß√µes exceto por uma, que representa a posi√ß√£o da palavra. Essa representa√ß√£o tem a disvantagem de n√£o transparecer qualquer semelhan√ßa entre palavras pr√≥ximas como "gato" e "felino" ou "amar" e "amor". Seria poss√≠vel, entretanto, criar um mapeamento de um one-hot vector para algum tipo de representa√ß√£o que permitisse codificar as semelhan√ßas entre palavras.

## Representa√ß√µes distribu√≠das

Um alternativa √†s representa√ß√µes discretas √© a ideia de poder codificar a semelhan√ßa entre palavras atrav√©s da pr√≥pria representa√ß√£o. A ideia por tr√°s desse tipo de representa√ß√£o √© a de que o significado de uma palavra est√° intimamente ligado ao contexto que √© mais prov√°vel de aparecer com essa palavra. N√≥s apresentaremos dois desses modelos: word2vec e GloVe.

## word2vec

O word2vec √© um modelo que representa as palavras como vetores que s√£o capazes de prever entre uma palavra e seu contexto. A ideia √© atualizar os vetores de cada palavra at√© que seja poss√≠vel prever uma palavra que acontece no contexto de uma palvra central, ou prever uma palavra central para um dado contexto. H√°, portanto, uma fun√ß√£o J(Œ∏) e o objetivo do word2vec √© minimizar essa fun√ß√£o.

### Skip-gram

### CBOW

// ...

## GloVe

// ...

## Compara√ß√£o

// resumo das descri√ß√µes acima salientando as diferen√ßas

### Avalia√ß√£o intr√≠nseca 

// dados de analogia do google

### Avalia√ß√£o extr√≠nseca

// alguma task de nlp usando os dois e gr√°ficos comparando

// na aula eles falaram que An√°lise de sentimento seria ruim üòû